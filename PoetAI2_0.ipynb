{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3XnXSj0a9lOkO5O/Nuq6m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/embaya01/PoetAI/blob/main/PoetAI2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9L_-M7mxvevE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and load in data from NLTK"
      ],
      "metadata": {
        "id": "K17LcEbC7HCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk_corpus = nltk.corpus.gutenberg\n",
        "corpus_name = 'melville-moby_dick.txt'\n",
        "tokens = list(nltk_corpus.words(corpus_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHhfjzL_vnQx",
        "outputId": "0d39559a-e00e-43af-cc65-38b1658ddcfd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define vocabulary and word-to-index mapping"
      ],
      "metadata": {
        "id": "HDhi68Oa7Rus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5000\n",
        "word_counter = Counter(tokens)\n",
        "vocab = {word[0]: idx+1 for idx, word in enumerate(word_counter.most_common(vocab_size))}\n",
        "vocab['<unknown>'] = 0\n",
        "word_to_idx = lambda word: vocab.get(word, vocab['<unknown>'])"
      ],
      "metadata": {
        "id": "Baz3ynRYvqzQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the sequence length"
      ],
      "metadata": {
        "id": "110Ge6NL7adQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 50"
      ],
      "metadata": {
        "id": "rar4QViDvtZk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create input and output sequences"
      ],
      "metadata": {
        "id": "2jTQbu227enI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_seqs = []\n",
        "output_seqs = []\n",
        "for i in range(0, len(tokens) - seq_length, 1):\n",
        "    input_seq = tokens[i:i + seq_length]\n",
        "    input_seqs.append([word_to_idx(word) for word in input_seq])\n",
        "    output_seq = tokens[i + seq_length]\n",
        "    output_seqs.append(word_to_idx(output_seq))"
      ],
      "metadata": {
        "id": "gOiDB3AAvwHc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert input and output sequences to numpy arrays"
      ],
      "metadata": {
        "id": "fvK9TMcS7k9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.reshape(input_seqs, (len(input_seqs), seq_length, 1))\n",
        "X = X / float(len(vocab))\n",
        "y = np_utils.to_categorical(output_seqs)"
      ],
      "metadata": {
        "id": "SI66cWeDvzym"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert y to a sparse matrix to save memory"
      ],
      "metadata": {
        "id": "xPyuxTku7qbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_data = np.ones(len(output_seqs))\n",
        "y_indices = output_seqs\n",
        "y_indptr = np.arange(len(output_seqs) + 1)\n",
        "num_unique_words = len(vocab)\n",
        "y = csr_matrix((y_data, y_indices, y_indptr), shape=(len(input_seqs), num_unique_words))"
      ],
      "metadata": {
        "id": "ZV8KD54Hv3UR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define/Train the LSTM model"
      ],
      "metadata": {
        "id": "fsdi1G2y7ux8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required Keras modules"
      ],
      "metadata": {
        "id": "0MnpF00U7yof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "t2UZq7qlv6mY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.toarray()"
      ],
      "metadata": {
        "id": "MWHQMOGWzbmY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "AKcTtzC7v9SV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a checkpoint to save the best model during training"
      ],
      "metadata": {
        "id": "6FVVL_LL79cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"best-model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "G0OZDjsQwAg8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=50, batch_size=16, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "0kmnCHYdwDlq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}